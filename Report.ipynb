{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project - Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, a solution to the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) is provided. Specifically, a Deep-Q-network is trained to play the Unity-Banana game and maximize the reward score.\n",
    "\n",
    "This jupyter notebook is designed to serve as report for the project, by providing all necessary details of the learning algorithm used.\n",
    "At the same time, the notebook also serves as training/testing of the agent to play the game.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- A Deep Q-Network (DQN) learning algorithm is implemented using pytorch to perform the given task.\n",
    "\n",
    "- The DQN has 3 fully connected layers with ReLU activation function, where `input_size` is the number of states\n",
    "    in the environment, here `37`. `output_size` is the number of actions possible for the agent; here `4`.\n",
    "    `fc1_units` and `fc2_units` are both taken as `64` in this project. The activation function for the\n",
    "    last layer is softmax, to ensure output probability sums up to 1.\n",
    "    ```\n",
    "        ('fc1', nn.Linear(input_size, fc1_units))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc2', nn.Linear(fc1_units, fc2_units))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc3', nn.Linear(fc2_units, output_size))\n",
    "    ```\n",
    "\n",
    "    The Q-network is tasked with the objective to find the optimal policy π* for the agent.\n",
    "    ```\n",
    "    Q(s,a) = Q(s, a) + α[R(s, a) + γmaxQ'(s',a')-Q(s,a)]\n",
    "    ```\n",
    "    \n",
    "- Prioritized Experience Replay: A buffer is implemented to provide sufficient experiences to\n",
    "    the agent from its memory. This helps re-using the experiences as the agent gets trained.\n",
    "    Furthermore, keeping high priority on re-learning experiences from memory with high td error\n",
    "    helps the agent learn more from useful experiences than otherwise.\n",
    "\n",
    "- Fixed target implementation: A double DQN approach is also adopted where the 2 sets of DQN\n",
    "    are maintained by the agent i.e. local and target. The local network is the one trained at\n",
    "    specific intervals. However, for evaluation of the agent's performance is based on the target\n",
    "    network which slowly converges to local network. This approach ensures that the recently\n",
    "    learned info by the local network doesn't disrupt the overall performance of the agent.\n",
    "    Furthermore, the approach makes the network learn slowly but tends to converge to learn correct\n",
    "    experiences over long periods.\n",
    "\n",
    "- Soft update: To update the weights of the target network, a simple weighted average between\n",
    "    the local and the target network is performed periodically. Depending on the weight, the\n",
    "    target network undergoes a soft update with respect to the high-frequency varying local\n",
    "    network.\n",
    "    \n",
    "- Hyperparameters: Several hyperparameters are used in the training/testing process, details\n",
    "    of which are given below. Further, the specific values of the parameters used during\n",
    "    the training session are stored in `log_path` file. Similarly, the graph of score vs episode\n",
    "    is stored as `fig_path` file. Users are requested to examine these two files for the training\n",
    "    performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup parameters (User-input required)\n",
    "\n",
    "Tune the following parameters to obtain necessary results.\n",
    "\n",
    "NOTE: Ensure `train_flag=False` and that `checkpoint.pth` file is available in the current folder if an already trained model is to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "train_flag = True # Set True if wish to train a network, else set False for testing\n",
    "SEED = 5 # Set user-defined seed to ensure same results, else set False for random seed\n",
    "\n",
    "priority_flag = False # Set True if wish to use priority sampling from Replay Buffer for training\n",
    "LR_decay_flag = False # Set True if wish to decay learning rate while training\n",
    "\n",
    "# Environment parameters\n",
    "qualify_score = 13 # Score at which the training must stop\n",
    "score_window = 100 # Number of episodes for which the qualify score should be maintained as average\n",
    "\n",
    "# QNN parameters\n",
    "BATCH_SIZE = 32 # Number of samples/batch for training the Q network\n",
    "LR = 1e-4 # Learning rate\n",
    "\n",
    "LR_DECAY_RATE = 0.99 # Learning rate decay rate\n",
    "LR_DECAY_STEP = int(BATCH_SIZE/5) # Learning rate decay step\n",
    "\n",
    "FC1 = 64 # Neurons in the first fully connected layer\n",
    "FC2 = 64 # Neurons in the second fully connected layer\n",
    "\n",
    "# DRL parameters\n",
    "steps_max = 500 # Maximum number of steps to be taken in an episode\n",
    "train_episodes = 1024 # 2048 # Number of episodes for which the agent must be trained (set as 50*BATCH_SIZE+1)\n",
    "\n",
    "eps_init = 1.0 # Initial epsilon for epsilon greedy\n",
    "eps_decay = 0.996 # The value by which the initial epsilon must decay over-time\n",
    "eps_min = 0.01 # The minimum value of epsilon beyond which there should be no decay\n",
    "\n",
    "SAMPLE_IMP = 1 # Amount of importance for prioritized sampling to those with high td estimate\n",
    "TAU = 1e-3 # The degree of influence the target network has on the main/local network\n",
    "UPDATE_EVERY = 4 # Number of time-steps in an episode after which the Q network should be updated\n",
    "\n",
    "BUFFER_SIZE = min([steps_max*train_episodes, 2**15]) # Number of episodes to keep in memory (experience replay)\n",
    "GAMMA = 0.98 # Discount factor of the rewards\n",
    "\n",
    "# Extra variables for ease of use\n",
    "app_path = 'Banana_Linux/Banana.x86_64'\n",
    "model_path = 'checkpoint.pth'\n",
    "\n",
    "log_path = 'output.txt'\n",
    "fig_path = 'output.pdf'\n",
    "loss_flag = False\n",
    "\n",
    "# # Values for input normalization - this step doesn't seem to help much\n",
    "# MAX_LINEAR_VEL = 12.5 # rough estimation for NN input normalization\n",
    "# MAX_ANGULAR_VEL = 4.0 # rough estimation for NN input normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# NN\n",
    "import torch as tc\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Replay buffer\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup the Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "env = UnityEnvironment(file_name=app_path)\n",
    "device = tc.device(\"cpu\") # Found CPU is good enough for this simple training\n",
    "# device = tc.device(\"cuda:0\" if tc.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get environment details\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Deep Q-Network\n",
    "\n",
    "The following class builds a generic NN model with only FC layers and ReLU activation function (using `pytorch`).\n",
    "The size and number of layers are tunable.\n",
    "\n",
    "Later in Sectio 5, this class is used to create DQN using the following architecture:\n",
    "    - Three fully connected layers with ReLU activation function\n",
    "    - Input layer is of size `state_size` (i.e. the number of state variables, 37 in our case)\n",
    "    - Second layer is of size `fc1_units`\n",
    "    - Third layer is of size `fc2_units`\n",
    "    - Output layer is of size `action_size` (i.e. the number of possible actions, 4 in our case)\n",
    "\n",
    "Also, later in Section 8, model parameters are loaded for testing if `train_flag=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a standard model\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, fc_units=[16], seed=False):\n",
    "        \"\"\"\n",
    "        Build a simple Fully Connected neural network with n hidden layers and ReLU activation.\n",
    "\n",
    "        Param\n",
    "        =====\n",
    "            input_size (int): Dimension of input layer\n",
    "            output_size (int): Dimension of output layer\n",
    "            fc_units (list of int): Number of nodes in each hidden layer, also setting the number of hidden layers\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        if seed:\n",
    "            self.seed = tc.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        for i in range(len(fc_units)-1):\n",
    "            self.model = nn.Sequential(\n",
    "                self.model,\n",
    "                nn.Linear(fc_units[i], fc_units[i+1]),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.model,\n",
    "            nn.Linear(fc_units[len(fc_units)-1], output_size)\n",
    "#             ,\n",
    "#             nn.Softmax(dim=1) # This tends to decrease learning performance\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Return a forward pass on the input\n",
    "        \n",
    "        Param\n",
    "        =====\n",
    "            input (list of size input_size_): Input to the model\n",
    "        \n",
    "        Return\n",
    "        =====\n",
    "            output from model (list of size output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Replay Buffer\n",
    "\n",
    "Experience replay is a key technique behind many recent advances in deep reinforcement learning.\n",
    "Allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlations.\n",
    "A replay buffer stores all the past memories upto `relay_buffer_size` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", \\\n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = np.random.seed(seed)\n",
    "        \n",
    "        if priority_flag:\n",
    "            self.priority=deque(maxlen=buffer_size)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, td_error=0.0):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "        if priority_flag:\n",
    "            priority = max([td_error, 1e-10]) # Maintain non-zero priority\n",
    "            self.priority.append(priority)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "\n",
    "        len_memory = len(self.memory)\n",
    "        list_memory = list(self.memory)\n",
    "        if priority_flag:\n",
    "            # Priority sampling\n",
    "            priorities = np.array(self.priority)**SAMPLE_IMP\n",
    "            sampling_prob = list( priorities / np.sum(priorities) )\n",
    "            indices = np.random.choice(len_memory, size=self.batch_size, p=sampling_prob)\n",
    "            experiences = [list_memory[index] for index in indices]\n",
    "        else:\n",
    "#             # Random sampling\n",
    "            indices = np.random.randint(0, len_memory-1, self.batch_size)\n",
    "            experiences = [list_memory[index] for index in indices]\n",
    "#             experiences = random.sample(self.memory, k=self.batch_size) # DEBUG:\n",
    "        \n",
    "        states = tc.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = tc.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = tc.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = tc.from_numpy(np.vstack([e.next_state for e in experiences \\\n",
    "                                                  if e is not None])).float().to(device)\n",
    "        dones = tc.from_numpy(np.vstack([e.done for e in experiences \\\n",
    "                                            if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Agent\n",
    "\n",
    "Create an agent to play the game and learn from it.\n",
    "Notice that the agent uses both of the above defined classes to create required objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = np.random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "#         self.dqn_local = DQN(state_size, action_size, [FC1, FC2], SEED).to(device)\n",
    "#         self.dqn_target = DQN(state_size, action_size, [FC1, FC2], SEED).to(device)\n",
    "        self.dqn_local = DQN(state_size, action_size, [64, 48, 24], SEED).to(device)\n",
    "        self.dqn_target = DQN(state_size, action_size, [64, 48, 24], SEED).to(device)\n",
    "        self.optimizer = optim.Adam(self.dqn_local.parameters(), lr=LR)\n",
    "        \n",
    "        if LR_decay_flag:\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=LR_DECAY_STEP, \\\n",
    "                                                       gamma=LR_DECAY_RATE)\n",
    "            self.scheduler_epoch = 0 # Initialize epoch (episode) step\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        if loss_flag:\n",
    "            self.loss_record = []\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "#         # State normalization (i.e. input to DQN) - this step doesn't seem to help much\n",
    "#         state = np.array(state) # Copy value, don't use reference\n",
    "#         next_state = np.array(next_state) # Copy value, don't use reference\n",
    "#         state[35] /= MAX_ANGULAR_VEL\n",
    "#         state[36] /= MAX_LINEAR_VEL\n",
    "#         next_state[35] /= MAX_ANGULAR_VEL\n",
    "#         next_state[36] /= MAX_LINEAR_VEL\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        if priority_flag:\n",
    "            # Priority sampling\n",
    "            td_error = abs(reward + GAMMA * max(self.evaluate(next_state)) - self.evaluate(state)[action])\n",
    "            self.memory.add(state, action, reward, next_state, done, td_error)\n",
    "        else:\n",
    "            # Random sampling\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "#         # State normalization (i.e. input to DQN) - this step doesn't seem to help much\n",
    "#         state = np.array(state) # Copy value, don't use reference\n",
    "#         state[35] /= MAX_ANGULAR_VEL\n",
    "#         state[36] /= MAX_LINEAR_VEL\n",
    "        \n",
    "        state = tc.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.dqn_local.eval()\n",
    "        with tc.no_grad():\n",
    "            action_values = self.dqn_local(state)\n",
    "        self.dqn_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        \"\"\"\n",
    "        Returns model output for given state.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Send to torch\n",
    "        state = tc.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get action values\n",
    "        self.dqn_local.eval()\n",
    "        with tc.no_grad():\n",
    "            action_values = self.dqn_local(state)\n",
    "        self.dqn_local.train()\n",
    "\n",
    "        return action_values.cpu().data.numpy()[0]\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.dqn_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.dqn_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # DEBUG: Plot loss\n",
    "        if loss_flag:\n",
    "            self.loss_record.append(loss.detach().numpy())\n",
    "            fig = plt.figure()\n",
    "            plt.plot(np.arange(len(self.loss_record)), self.loss_record)\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('Batch #')\n",
    "            plt.grid()\n",
    "            fig.savefig('loss_record.pdf', bbox_inches='tight') # Vector graphics for detailed viz\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.dqn_local, self.dqn_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Train the network (only if `train_flag=True`)\n",
    "\n",
    "NOTE: Training results are stored as `log_path` and the graph as `fig_path` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Episode: 100 Step: 250 Yellow Score: 00 Blue Score: 01 Score: -1.0\n",
      "Episode: 100 Avg Time: 1.18s Avg Score: 0.94\n",
      "\t Episode: 110 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 110 Avg Time: 1.18s Avg Score: 1.09\n",
      "\t Episode: 120 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 120 Avg Time: 1.18s Avg Score: 1.23\n",
      "\t Episode: 130 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 130 Avg Time: 1.18s Avg Score: 1.38\n",
      "\t Episode: 140 Step: 250 Yellow Score: 02 Blue Score: 01 Score: 1.00\n",
      "Episode: 140 Avg Time: 1.18s Avg Score: 1.48\n",
      "\t Episode: 150 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 150 Avg Time: 1.18s Avg Score: 1.61\n",
      "\t Episode: 160 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 160 Avg Time: 1.18s Avg Score: 1.68\n",
      "\t Episode: 170 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 170 Avg Time: 1.18s Avg Score: 1.91\n",
      "\t Episode: 180 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 180 Avg Time: 1.18s Avg Score: 2.2\n",
      "\t Episode: 190 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 190 Avg Time: 1.17s Avg Score: 2.2\n",
      "\t Episode: 200 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 200 Avg Time: 1.17s Avg Score: 2.31\n",
      "\t Episode: 210 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 210 Avg Time: 1.17s Avg Score: 2.52\n",
      "\t Episode: 220 Step: 250 Yellow Score: 03 Blue Score: 02 Score: 1.00\n",
      "Episode: 220 Avg Time: 1.17s Avg Score: 2.71\n",
      "\t Episode: 230 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 230 Avg Time: 1.17s Avg Score: 2.85\n",
      "\t Episode: 240 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.0\n",
      "Episode: 240 Avg Time: 1.17s Avg Score: 3.1\n",
      "\t Episode: 250 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 250 Avg Time: 1.17s Avg Score: 3.24\n",
      "\t Episode: 260 Step: 250 Yellow Score: 04 Blue Score: 02 Score: 2.00\n",
      "Episode: 260 Avg Time: 1.18s Avg Score: 3.38\n",
      "\t Episode: 270 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 270 Avg Time: 1.18s Avg Score: 3.46\n",
      "\t Episode: 280 Step: 250 Yellow Score: 06 Blue Score: 02 Score: 4.00\n",
      "Episode: 280 Avg Time: 1.18s Avg Score: 3.39\n",
      "\t Episode: 290 Step: 250 Yellow Score: 02 Blue Score: 02 Score: 0.00\n",
      "Episode: 290 Avg Time: 1.18s Avg Score: 3.53\n",
      "\t Episode: 300 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 300 Avg Time: 1.18s Avg Score: 3.6\n",
      "\t Episode: 310 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.0\n",
      "Episode: 310 Avg Time: 1.18s Avg Score: 3.61\n",
      "\t Episode: 320 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 320 Avg Time: 1.18s Avg Score: 3.65\n",
      "\t Episode: 330 Step: 250 Yellow Score: 09 Blue Score: 01 Score: 8.00\n",
      "Episode: 330 Avg Time: 1.18s Avg Score: 3.85\n",
      "\t Episode: 340 Step: 250 Yellow Score: 08 Blue Score: 01 Score: 7.00\n",
      "Episode: 340 Avg Time: 1.18s Avg Score: 3.85\n",
      "\t Episode: 350 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 350 Avg Time: 1.18s Avg Score: 3.85\n",
      "\t Episode: 360 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 360 Avg Time: 1.19s Avg Score: 3.99\n",
      "\t Episode: 370 Step: 250 Yellow Score: 05 Blue Score: 01 Score: 4.00\n",
      "Episode: 370 Avg Time: 1.19s Avg Score: 3.94\n",
      "\t Episode: 380 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 380 Avg Time: 1.19s Avg Score: 4.01\n",
      "\t Episode: 390 Step: 250 Yellow Score: 01 Blue Score: 01 Score: 0.0\n",
      "Episode: 390 Avg Time: 1.19s Avg Score: 4.02\n",
      "\t Episode: 400 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 400 Avg Time: 1.19s Avg Score: 3.95\n",
      "\t Episode: 410 Step: 250 Yellow Score: 07 Blue Score: 00 Score: 7.00\n",
      "Episode: 410 Avg Time: 1.19s Avg Score: 3.9\n",
      "\t Episode: 420 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 420 Avg Time: 1.19s Avg Score: 3.89\n",
      "\t Episode: 430 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 430 Avg Time: 1.19s Avg Score: 3.72\n",
      "\t Episode: 440 Step: 250 Yellow Score: 02 Blue Score: 01 Score: 1.00\n",
      "Episode: 440 Avg Time: 1.19s Avg Score: 3.81\n",
      "\t Episode: 450 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 450 Avg Time: 1.19s Avg Score: 3.77\n",
      "\t Episode: 460 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 460 Avg Time: 1.19s Avg Score: 3.69\n",
      "\t Episode: 470 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 470 Avg Time: 1.19s Avg Score: 3.53\n",
      "\t Episode: 480 Step: 250 Yellow Score: 00 Blue Score: 00 Score: 0.00\n",
      "Episode: 480 Avg Time: 1.20s Avg Score: 3.53\n",
      "\t Episode: 490 Step: 250 Yellow Score: 00 Blue Score: 00 Score: 0.00\n",
      "Episode: 490 Avg Time: 1.20s Avg Score: 3.61\n",
      "\t Episode: 500 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 500 Avg Time: 1.20s Avg Score: 3.65\n",
      "\t Episode: 510 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 510 Avg Time: 1.20s Avg Score: 3.62\n",
      "\t Episode: 520 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 520 Avg Time: 1.20s Avg Score: 3.77\n",
      "\t Episode: 530 Step: 250 Yellow Score: 02 Blue Score: 03 Score: -1.0\n",
      "Episode: 530 Avg Time: 1.20s Avg Score: 3.71\n",
      "\t Episode: 540 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.0\n",
      "Episode: 540 Avg Time: 1.20s Avg Score: 3.75\n",
      "\t Episode: 550 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 550 Avg Time: 1.20s Avg Score: 3.9\n",
      "\t Episode: 560 Step: 250 Yellow Score: 01 Blue Score: 01 Score: 0.0\n",
      "Episode: 560 Avg Time: 1.20s Avg Score: 3.86\n",
      "\t Episode: 570 Step: 250 Yellow Score: 04 Blue Score: 02 Score: 2.00\n",
      "Episode: 570 Avg Time: 1.20s Avg Score: 4.02\n",
      "\t Episode: 580 Step: 250 Yellow Score: 05 Blue Score: 02 Score: 3.00\n",
      "Episode: 580 Avg Time: 1.20s Avg Score: 3.98\n",
      "\t Episode: 590 Step: 250 Yellow Score: 07 Blue Score: 01 Score: 6.00\n",
      "Episode: 590 Avg Time: 1.21s Avg Score: 4.11\n",
      "\t Episode: 600 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 600 Avg Time: 1.21s Avg Score: 4.23\n",
      "\t Episode: 610 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 610 Avg Time: 1.21s Avg Score: 4.33\n",
      "\t Episode: 620 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 620 Avg Time: 1.21s Avg Score: 4.4\n",
      "\t Episode: 630 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 630 Avg Time: 1.21s Avg Score: 4.42\n",
      "\t Episode: 640 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 640 Avg Time: 1.21s Avg Score: 4.28\n",
      "\t Episode: 650 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 650 Avg Time: 1.21s Avg Score: 4.28\n",
      "\t Episode: 660 Step: 250 Yellow Score: 03 Blue Score: 03 Score: 0.00\n",
      "Episode: 660 Avg Time: 1.21s Avg Score: 4.44\n",
      "\t Episode: 670 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 670 Avg Time: 1.21s Avg Score: 4.49\n",
      "\t Episode: 680 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 680 Avg Time: 1.21s Avg Score: 4.52\n",
      "\t Episode: 690 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 690 Avg Time: 1.21s Avg Score: 4.46\n",
      "\t Episode: 700 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 700 Avg Time: 1.21s Avg Score: 4.47\n",
      "\t Episode: 710 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 710 Avg Time: 1.21s Avg Score: 4.49\n",
      "\t Episode: 720 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 720 Avg Time: 1.22s Avg Score: 4.38\n",
      "\t Episode: 730 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 730 Avg Time: 1.22s Avg Score: 4.51\n",
      "\t Episode: 740 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 740 Avg Time: 1.22s Avg Score: 4.56\n",
      "\t Episode: 750 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 750 Avg Time: 1.22s Avg Score: 4.52\n",
      "\t Episode: 760 Step: 250 Yellow Score: 05 Blue Score: 01 Score: 4.00\n",
      "Episode: 760 Avg Time: 1.22s Avg Score: 4.62\n",
      "\t Episode: 770 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 770 Avg Time: 1.22s Avg Score: 4.57\n",
      "\t Episode: 780 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 780 Avg Time: 1.22s Avg Score: 4.64\n",
      "\t Episode: 790 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.00\n",
      "Episode: 790 Avg Time: 1.22s Avg Score: 4.64\n",
      "\t Episode: 800 Step: 250 Yellow Score: 00 Blue Score: 00 Score: 0.00\n",
      "Episode: 800 Avg Time: 1.22s Avg Score: 4.53\n",
      "\t Episode: 810 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 810 Avg Time: 1.22s Avg Score: 4.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Episode: 820 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 820 Avg Time: 1.22s Avg Score: 4.59\n",
      "\t Episode: 830 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 2.00\n",
      "Episode: 830 Avg Time: 1.22s Avg Score: 4.56\n",
      "\t Episode: 840 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 840 Avg Time: 1.22s Avg Score: 4.58\n",
      "\t Episode: 850 Step: 250 Yellow Score: 06 Blue Score: 00 Score: 6.00\n",
      "Episode: 850 Avg Time: 1.22s Avg Score: 4.59\n",
      "\t Episode: 860 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 860 Avg Time: 1.22s Avg Score: 4.38\n",
      "\t Episode: 870 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 870 Avg Time: 1.22s Avg Score: 4.53\n",
      "\t Episode: 880 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 880 Avg Time: 1.22s Avg Score: 4.42\n",
      "\t Episode: 890 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 890 Avg Time: 1.22s Avg Score: 4.46\n",
      "\t Episode: 900 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 900 Avg Time: 1.22s Avg Score: 4.63\n",
      "\t Episode: 910 Step: 250 Yellow Score: 06 Blue Score: 01 Score: 5.00\n",
      "Episode: 910 Avg Time: 1.22s Avg Score: 4.58\n",
      "\t Episode: 920 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 920 Avg Time: 1.22s Avg Score: 4.66\n",
      "\t Episode: 930 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 930 Avg Time: 1.22s Avg Score: 4.75\n",
      "\t Episode: 940 Step: 250 Yellow Score: 06 Blue Score: 00 Score: 6.0\n",
      "Episode: 940 Avg Time: 1.22s Avg Score: 4.78\n",
      "\t Episode: 950 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 950 Avg Time: 1.22s Avg Score: 4.84\n",
      "\t Episode: 960 Step: 250 Yellow Score: 00 Blue Score: 01 Score: -1.0\n",
      "Episode: 960 Avg Time: 1.22s Avg Score: 4.95\n",
      "\t Episode: 970 Step: 250 Yellow Score: 02 Blue Score: 01 Score: 1.00\n",
      "Episode: 970 Avg Time: 1.22s Avg Score: 4.73\n",
      "\t Episode: 980 Step: 250 Yellow Score: 07 Blue Score: 00 Score: 7.00\n",
      "Episode: 980 Avg Time: 1.22s Avg Score: 4.85\n",
      "\t Episode: 990 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 990 Avg Time: 1.22s Avg Score: 4.86\n",
      "\t Episode: 1000 Step: 250 Yellow Score: 03 Blue Score: 00 Score: 3.0\n",
      "Episode: 1000 Avg Time: 1.22s Avg Score: 4.67\n",
      "\t Episode: 1010 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 1010 Avg Time: 1.22s Avg Score: 4.7\n",
      "\t Episode: 1020 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 1020 Avg Time: 1.22s Avg Score: 4.59\n",
      "\t Episode: 1023 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.0\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an agent which interacts with the environment\n",
    "agent = Agent(state_size, action_size, SEED)\n",
    "\n",
    "if train_flag:\n",
    "\n",
    "    # Setup variables\n",
    "    all_scores = []\n",
    "    steps_period = int(steps_max/10)\n",
    "    episode_period = min([int(train_episodes/100), score_window])\n",
    "    start_time = time()\n",
    "\n",
    "    # Log info (such that browser can be safely closed)\n",
    "    text = 'priority_flag = {}\\nLR_decay_flag = {}\\nqualify_score = {}\\nscore_window = {}\\nBATCH_SIZE = {}\\n' + \\\n",
    "            'LR = {}\\nLR_DECAY_RATE = {}\\nLR_DECAY_STEP = {}\\nFC1 = {}\\nFC2 = {}\\ntrain_episodes = {}\\n' + \\\n",
    "            'steps_max = {}\\neps_init = {}\\neps_decay = {}\\neps_min = {}\\nSAMPLE_IMP = {}\\nTAU = {}\\n' + \\\n",
    "            'UPDATE_EVERY = {}\\nBUFFER_SIZE = {}\\nGAMMA = {}\\n\\n'\n",
    "    text = text.format(priority_flag, LR_decay_flag, qualify_score, score_window, BATCH_SIZE, LR, LR_DECAY_RATE,\\\n",
    "                       LR_DECAY_STEP, FC1, FC2, train_episodes, steps_max, eps_init, eps_decay, eps_min, \\\n",
    "                       SAMPLE_IMP, TAU, UPDATE_EVERY, BUFFER_SIZE,GAMMA)\n",
    "    with open(log_path,'w+') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    for e in range(train_episodes):\n",
    "\n",
    "        # Reset environment and setup variables\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0] # get the current state\n",
    "        score = 0 # initialize the score\n",
    "        yellow_bananas = 0\n",
    "        blue_bananas = 0\n",
    "        eps = eps_init # Initialize epsilon\n",
    "        solved_flag = False\n",
    "        if LR_decay_flag:\n",
    "            agent.scheduler_epoch = e\n",
    "\n",
    "        for t in range(steps_max):\n",
    "            # Get next action from the agent\n",
    "            action = agent.act(state, eps)\n",
    "\n",
    "            # Run one step and collect env info\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            if reward == 1:\n",
    "                yellow_bananas += 1\n",
    "            elif reward == -1:\n",
    "                blue_bananas += 1\n",
    "            score += reward\n",
    "\n",
    "            # Check if required average score is obtained\n",
    "            if t % steps_period == 0:\n",
    "                text = \"\\t Episode: {} Step: {:03} Yellow Score: {:02} Blue Score: {:02} Score: {:02}\".format(\\\n",
    "                    e, t, yellow_bananas, blue_bananas, score)\n",
    "                print(text, end = '\\r')\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "            if done:\n",
    "                    all_scores.append(score)\n",
    "                    break\n",
    "\n",
    "            # Prepare for the next step\n",
    "            agent.step(state, action, reward, next_state, done) # Store the information of last step\n",
    "            state = next_state # Update state\n",
    "            eps = max(eps_min, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "\n",
    "        # Check if required average score is obtained\n",
    "        if len(all_scores) > score_window:\n",
    "            if e % episode_period == 0:\n",
    "                text = \"Episode: {} Avg Time: {:0.2f}s Avg Score: {}\".format(e, (time()-start_time)/e, \\\n",
    "                                                                            np.mean(all_scores[-score_window:]))\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "                print()\n",
    "                print(text)\n",
    "                \n",
    "                tc.save(agent.dqn_local.state_dict(), 'checkpoint_unoptimized.pth')\n",
    "\n",
    "                # Plot the scores - for better viz, than just texts\n",
    "                fig = plt.figure()\n",
    "                plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "                plt.ylabel('Score')\n",
    "                plt.xlabel('Episode #')\n",
    "                plt.grid()\n",
    "                fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "                plt.close(fig)\n",
    "                \n",
    "            if np.mean(all_scores[-score_window:]) > qualify_score:\n",
    "                text = \"Solved environment in {} episodes, avg score: {}\".format(\\\n",
    "                    e, np.mean(all_scores[-score_window:]))\n",
    "\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "                \n",
    "                print(text)\n",
    "                \n",
    "                tc.save(agent.dqn_local.state_dict(), 'checkpoint.pth')\n",
    "                \n",
    "                solved_flag = True\n",
    "                \n",
    "                break\n",
    "                \n",
    "        \n",
    "    # Plot the scores\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    if solved_flag:\n",
    "        plt.title(text)\n",
    "    else:\n",
    "        plt.title(text[2:])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "    fig.savefig('rewards.png', bbox_inches='tight') # Raster graphics to show later in Section 9\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Test the performance (only if `train_flag=False`)\n",
    "\n",
    "Test an already trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_flag:\n",
    "    \n",
    "    # Initialization\n",
    "    agent.dqn_local.load_state_dict(tc.load('checkpoint.pth')) # load the weights from file\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0] # get the current state\n",
    "    score = 0 # initialize the score\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state,0.0) # select an action\n",
    "        env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0] # get the next state\n",
    "        reward = env_info.rewards[0] # get the reward\n",
    "        done = env_info.local_done[0] # see if episode has finished\n",
    "        score += reward # update the score\n",
    "        state = next_state # roll over the state to next time step\n",
    "        if done: # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### Plot of training rewards\n",
    "\n",
    "![banana_agent_rewards](rewards.png)\n",
    "\n",
    "NOTE: The above figure might not be the true figure due to disk cache problem. Please see `fig_path` file for the latest and true graph.\n",
    "\n",
    "### Observations\n",
    "\n",
    "There were a lot of observations and learnings during the training sessions:\n",
    "- In this work, a Double DQN approach with Prioritized Experience Replay Buffer is implemented. This is because from personal exploration, a simple DQN didn't seem to train steadily i.e. avg score kept fluctuating a lot. Furthermore, in general, a random sampling in the replay buffer provided the torch optimizer with experience in which reward is typically 0 i.e. an experience which doesn't teach much useful information.\n",
    "- Introduction of softmax to the output of the neural network tremendously reduces the\n",
    "    performance of the learning. This is indeed surprising because typically the probabilistic output networks\n",
    "    have softmax to ensure that output is probabilistic (i.e. 1<p<1 and sum(p) = 1).\n",
    "- Input normalization, specifically the linear and angular velocity of the agent to [-1, 1], \n",
    "    doesn't seem to help with network learning.\n",
    "- Learning rate decay of the network optimizer doesn't seem to help in improving learning rate,\n",
    "    probably because this require precise tuning of the decay rate and initial value of the learning rate.\n",
    "- Prioritized Replay Buffer helps increase the rate of learning substantially, although at the cost of\n",
    "    computational time.\n",
    "  \n",
    "- The hyperparameters which suit well are designed such that the local network trains faster\n",
    "    but slightly unstably, while the target network trains slowly and tries to make the prediction\n",
    "    much more stable.\n",
    "- There were numerous cases where the training performance increased rapidly, then decreased, followed by increase again in the average score. Such a situation might arise due to fast learning approach such as target network is updated to local network very fast, while the local network is being trained with small batch size with minimal data.\n",
    "- For instance, in one of the sets of the training parameters, the avg score of +13 over 100 episodes was obtained within about 500 episodes. Although, the score graph was very noisy and the agent's performace during testing was very jerky, almost as if the agent was randomly choosing actions instead of continuously progressing towards a yellow banana.\n",
    "- In constrast to above, several cases where also observed where the system doesn't learn much even after thousands of training episodes, simply because the training parameters were tuned to learn slowly. Accordingly, it is learned that the training parameters can significantly affect the system performance, beyond the learning algorithm itself.\n",
    "- A lot of times, the average score increased and thereafter decrease after a number of episodes. This might be because the network is trained to learn faster, rather than learn a lot of good moves.\n",
    "- It is also learned that there needs to be a trade-off between larger buffer size against slower convergence of target network to the local network, because otherwise, the network might forget good experiences very easily and start to work on poor results.\n",
    "- Finally, a few trials with random initial network weights is a good approach to ensure trying out all potentially good possible results.\n",
    "\n",
    "### Future Ideas\n",
    "\n",
    "There were a lot of learning from our observations as mentioned above.\n",
    "Luckily, there does exists a lot of algorithms (such as Actor-Critic approach, DDPG, etc.) in the literature which focus on overcoming the short-coming of the Deep Q-Network approach implemented in this work.\n",
    "Hence, in future, it is worth exploring other algorithms for improving the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
