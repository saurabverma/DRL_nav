{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project - Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, a solution to the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) is provided. Specifically, a Deep-Q-network is trained to play the Unity-Banana game and maximize the reward score.\n",
    "\n",
    "This jupyter notebook is designed to serve as report for the project, by providing all necessary details of the learning algorithm used.\n",
    "At the same time, the notebook also serves as training/testing of the agent to play the game.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- A Deep Q-Network (DQN) learning algorithm is implemented using pytorch to perform the given task.\n",
    "\n",
    "- The DQN has 4 fully connected layers with ReLU activation function, where `input_size` is the number of states\n",
    "    in the environment, here `37`. `output_size` is the number of actions possible for the agent; here `4`.\n",
    "    In between, `FC` equal to `[64, 48, 24]` determines the size of the hidden layers in this project.\n",
    "    The activation function for the last layer is softmax, to ensure output probability sums up to 1.\n",
    "    ```\n",
    "        ('fc1', nn.Linear(input_size, FC[0]))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc2', nn.Linear(FC[0], FC[1]))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc3', nn.Linear(FC[1], FC[2]))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc4', nn.Linear(FC[2], output_size))\n",
    "    ```\n",
    "\n",
    "    The Q-network is tasked with the objective to find the optimal policy π* for the agent.\n",
    "    ```\n",
    "    Q(s,a) = Q(s, a) + α[R(s, a) + γmaxQ'(s',a')-Q(s,a)]\n",
    "    ```\n",
    "    \n",
    "- Prioritized Experience Replay: A buffer is implemented to provide sufficient experiences to\n",
    "    the agent from its memory. This helps re-using the experiences as the agent gets trained.\n",
    "    Furthermore, keeping high priority on re-learning experiences from memory with high td error\n",
    "    helps the agent learn more from useful experiences than otherwise.\n",
    "\n",
    "- Fixed target implementation: A double DQN approach is also adopted where the 2 sets of DQN\n",
    "    are maintained by the agent i.e. local and target. The local network is the one trained at\n",
    "    specific intervals. However, for evaluation of the agent's performance is based on the target\n",
    "    network which slowly converges to local network. This approach ensures that the recently\n",
    "    learned info by the local network doesn't disrupt the overall performance of the agent.\n",
    "    Furthermore, the approach makes the network learn slowly but tends to converge to learn correct\n",
    "    experiences over long periods.\n",
    "\n",
    "- Soft update: To update the weights of the target network, a simple weighted average between\n",
    "    the local and the target network is performed periodically. Depending on the weight, the\n",
    "    target network undergoes a soft update with respect to the high-frequency varying local\n",
    "    network.\n",
    "    \n",
    "- Hyperparameters: Several hyperparameters are used in the training/testing process, details\n",
    "    of which are given below. Further, the specific values of the parameters used during\n",
    "    the training session are stored in `log_path` file. Similarly, the graph of score vs episode\n",
    "    is stored as `fig_path` file. Users are requested to examine these two files for the training\n",
    "    performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup parameters (User-input required)\n",
    "\n",
    "Tune the following parameters to obtain necessary results.\n",
    "\n",
    "NOTE: Ensure `train_flag=False` and that `checkpoint.pth` file is available in the current folder if an already trained model is to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "train_flag = True # Set True if wish to train a network, else set False for testing\n",
    "SEED = 5 # Set user-defined seed to ensure same results, else set False for random seed\n",
    "\n",
    "priority_flag = True # Set True if wish to use priority sampling from Replay Buffer for training\n",
    "LR_decay_flag = False # Set True if wish to decay learning rate while training\n",
    "\n",
    "# Environment parameters\n",
    "qualify_score = 13 # Score at which the training must stop\n",
    "score_window = 100 # Number of episodes for which the qualify score should be maintained as average\n",
    "\n",
    "# QNN parameters\n",
    "FC = [64, 48, 24] # Neurons in the hidden fully connected layers\n",
    "BATCH_SIZE = 32 # Number of samples/batch for training the Q network\n",
    "\n",
    "LR = 1e-4 # Learning rate\n",
    "LR_DECAY_RATE = 0.99 # Learning rate decay rate\n",
    "LR_DECAY_STEP = int(BATCH_SIZE/5) # Learning rate decay step\n",
    "\n",
    "# DRL parameters\n",
    "steps_max = 500 # Maximum number of steps to be taken in an episode\n",
    "train_episodes = 2048 # Number of episodes for which the agent must be trained (set as 50*BATCH_SIZE+1)\n",
    "\n",
    "eps_init = 1.0 # Initial epsilon for epsilon greedy\n",
    "eps_decay = 0.94 # The value by which the initial epsilon must decay over-time\n",
    "eps_min = 0.001  # The minimum value of epsilon beyond which there should be no decay\n",
    "\n",
    "SAMPLE_IMP = 0.75 # Amount of importance for prioritized sampling to those with high td estimate\n",
    "TAU = 1e-3 # The degree of influence the target network has on the main/local network\n",
    "UPDATE_EVERY = 4 # Number of time-steps in an episode after which the Q network should be updated\n",
    "\n",
    "BUFFER_SIZE = min([steps_max*train_episodes, 2**15]) # Number of episodes to keep in memory (experience replay)\n",
    "GAMMA = 0.98 # Discount factor of the rewards\n",
    "\n",
    "# Extra variables for ease of use\n",
    "app_path = 'Banana_Linux/Banana.x86_64'\n",
    "model_path = 'checkpoint.pth'\n",
    "\n",
    "log_path = 'output.txt'\n",
    "fig_path = 'output.pdf'\n",
    "loss_flag = False\n",
    "\n",
    "# # Values for input normalization - this step doesn't seem to help much\n",
    "# MAX_LINEAR_VEL = 12.5 # rough estimation for NN input normalization\n",
    "# MAX_ANGULAR_VEL = 4.0 # rough estimation for NN input normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# NN\n",
    "import torch as tc\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Replay buffer\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup the Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "env = UnityEnvironment(file_name=app_path)\n",
    "device = tc.device(\"cpu\") # Found CPU is good enough for this simple training\n",
    "# device = tc.device(\"cuda:0\" if tc.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get environment details\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Deep Q-Network\n",
    "\n",
    "The following class builds a generic NN model with only FC layers and ReLU activation function (using `pytorch`).\n",
    "The size and number of layers are tunable.\n",
    "\n",
    "Later in Sectio 5, this class is used to create DQN using the following architecture:\n",
    "    - Three fully connected layers with ReLU activation function\n",
    "    - Input layer is of size `state_size` (i.e. the number of state variables, 37 in our case)\n",
    "    - Second layer is of size `FC[1]`\n",
    "    - Third layer is of size `FC[2]`\n",
    "    - Fourth layer is of size `FC[3]`\n",
    "    - Output layer is of size `action_size` (i.e. the number of possible actions, 4 in our case)\n",
    "\n",
    "Also, later in Section 8, model parameters are loaded for testing if `train_flag=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a standard model\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, fc_units=[16], seed=False):\n",
    "        \"\"\"\n",
    "        Build a simple Fully Connected neural network with n hidden layers and ReLU activation.\n",
    "\n",
    "        Param\n",
    "        =====\n",
    "            input_size (int): Dimension of input layer\n",
    "            output_size (int): Dimension of output layer\n",
    "            fc_units (list of int): Number of nodes in each hidden layer, also setting the number of hidden layers\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        if seed:\n",
    "            self.seed = tc.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        for i in range(len(fc_units)-1):\n",
    "            self.model = nn.Sequential(\n",
    "                self.model,\n",
    "                nn.Linear(fc_units[i], fc_units[i+1]),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.model,\n",
    "            nn.Linear(fc_units[len(fc_units)-1], output_size)\n",
    "#             ,\n",
    "#             nn.Softmax(dim=1) # This tends to decrease learning performance\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Return a forward pass on the input\n",
    "        \n",
    "        Param\n",
    "        =====\n",
    "            input (list of size input_size_): Input to the model\n",
    "        \n",
    "        Return\n",
    "        =====\n",
    "            output from model (list of size output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Replay Buffer\n",
    "\n",
    "Experience replay is a key technique behind many recent advances in deep reinforcement learning.\n",
    "Allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlations.\n",
    "A replay buffer stores all the past memories upto `relay_buffer_size` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", \\\n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = np.random.seed(seed)\n",
    "        \n",
    "        if priority_flag:\n",
    "            self.priority=deque(maxlen=buffer_size)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done, td_error=0.0):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "        if priority_flag:\n",
    "            priority = max([td_error, 1e-10]) # Maintain non-zero priority\n",
    "            self.priority.append(priority)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "\n",
    "        len_memory = len(self.memory)\n",
    "        list_memory = list(self.memory)\n",
    "        if priority_flag:\n",
    "            # Priority sampling\n",
    "            priorities = np.array(self.priority)**SAMPLE_IMP\n",
    "            sampling_prob = list( priorities / np.sum(priorities) )\n",
    "            indices = np.random.choice(len_memory, size=self.batch_size, p=sampling_prob)\n",
    "            experiences = [list_memory[index] for index in indices]\n",
    "        else:\n",
    "#             # Random sampling\n",
    "            indices = np.random.randint(0, len_memory-1, self.batch_size)\n",
    "            experiences = [list_memory[index] for index in indices]\n",
    "#             experiences = random.sample(self.memory, k=self.batch_size) # DEBUG:\n",
    "        \n",
    "        states = tc.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = tc.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = tc.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = tc.from_numpy(np.vstack([e.next_state for e in experiences \\\n",
    "                                                  if e is not None])).float().to(device)\n",
    "        dones = tc.from_numpy(np.vstack([e.done for e in experiences \\\n",
    "                                            if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Agent\n",
    "\n",
    "Create an agent to play the game and learn from it.\n",
    "Notice that the agent uses both of the above defined classes to create required objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = np.random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.dqn_local = DQN(state_size, action_size, FC, SEED).to(device)\n",
    "        self.dqn_target = DQN(state_size, action_size, FC, SEED).to(device)\n",
    "        self.optimizer = optim.Adam(self.dqn_local.parameters(), lr=LR)\n",
    "        \n",
    "        if LR_decay_flag:\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=LR_DECAY_STEP, \\\n",
    "                                                       gamma=LR_DECAY_RATE)\n",
    "            self.scheduler_epoch = 0 # Initialize epoch (episode) step\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        if loss_flag:\n",
    "            self.loss_record = []\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "#         # State normalization (i.e. input to DQN) - this step doesn't seem to help much\n",
    "#         state = np.array(state) # Copy value, don't use reference\n",
    "#         next_state = np.array(next_state) # Copy value, don't use reference\n",
    "#         state[35] /= MAX_ANGULAR_VEL\n",
    "#         state[36] /= MAX_LINEAR_VEL\n",
    "#         next_state[35] /= MAX_ANGULAR_VEL\n",
    "#         next_state[36] /= MAX_LINEAR_VEL\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        if priority_flag:\n",
    "            # Priority sampling\n",
    "            td_error = abs(reward + GAMMA * max(self.evaluate(next_state)) - self.evaluate(state)[action])\n",
    "            self.memory.add(state, action, reward, next_state, done, td_error)\n",
    "        else:\n",
    "            # Random sampling\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "#         # State normalization (i.e. input to DQN) - this step doesn't seem to help much\n",
    "#         state = np.array(state) # Copy value, don't use reference\n",
    "#         state[35] /= MAX_ANGULAR_VEL\n",
    "#         state[36] /= MAX_LINEAR_VEL\n",
    "        \n",
    "        state = tc.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.dqn_local.eval()\n",
    "        with tc.no_grad():\n",
    "            action_values = self.dqn_local(state)\n",
    "        self.dqn_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        \"\"\"\n",
    "        Returns model output for given state.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Send to torch\n",
    "        state = tc.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get action values\n",
    "        self.dqn_local.eval()\n",
    "        with tc.no_grad():\n",
    "            action_values = self.dqn_local(state)\n",
    "        self.dqn_local.train()\n",
    "\n",
    "        return action_values.cpu().data.numpy()[0]\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.dqn_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.dqn_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # DEBUG: Plot loss\n",
    "        if loss_flag:\n",
    "            self.loss_record.append(loss.detach().numpy())\n",
    "            fig = plt.figure()\n",
    "            plt.plot(np.arange(len(self.loss_record)), self.loss_record)\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('Batch #')\n",
    "            plt.grid()\n",
    "            fig.savefig('loss_record.pdf', bbox_inches='tight') # Vector graphics for detailed viz\n",
    "            plt.close(fig)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.dqn_local, self.dqn_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Train the network (only if `train_flag=True`)\n",
    "\n",
    "NOTE: Training results are stored as `log_path` and the graph as `fig_path` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Episode: 100 Step: 250 Yellow Score: 00 Blue Score: 00 Score: 0.0\n",
      "Episode: 100 Avg Time: 1.91s Avg Score: 1.8\n",
      "\t Episode: 120 Step: 250 Yellow Score: 04 Blue Score: 01 Score: 3.00\n",
      "Episode: 120 Avg Time: 1.95s Avg Score: 2.69\n",
      "\t Episode: 140 Step: 250 Yellow Score: 07 Blue Score: 01 Score: 6.00\n",
      "Episode: 140 Avg Time: 1.97s Avg Score: 3.82\n",
      "\t Episode: 160 Step: 250 Yellow Score: 08 Blue Score: 00 Score: 8.00\n",
      "Episode: 160 Avg Time: 1.99s Avg Score: 4.45\n",
      "\t Episode: 180 Step: 250 Yellow Score: 01 Blue Score: 00 Score: 1.00\n",
      "Episode: 180 Avg Time: 2.01s Avg Score: 4.81\n",
      "\t Episode: 200 Step: 250 Yellow Score: 08 Blue Score: 00 Score: 8.00\n",
      "Episode: 200 Avg Time: 2.03s Avg Score: 5.2\n",
      "\t Episode: 220 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 220 Avg Time: 2.04s Avg Score: 5.03\n",
      "\t Episode: 240 Step: 250 Yellow Score: 05 Blue Score: 00 Score: 5.00\n",
      "Episode: 240 Avg Time: 2.04s Avg Score: 4.78\n",
      "\t Episode: 260 Step: 250 Yellow Score: 04 Blue Score: 00 Score: 4.00\n",
      "Episode: 260 Avg Time: 2.05s Avg Score: 5.24\n",
      "\t Episode: 280 Step: 250 Yellow Score: 10 Blue Score: 00 Score: 10.0\n",
      "Episode: 280 Avg Time: 2.06s Avg Score: 5.78\n",
      "\t Episode: 300 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 300 Avg Time: 2.06s Avg Score: 6.91\n",
      "\t Episode: 320 Step: 250 Yellow Score: 09 Blue Score: 00 Score: 9.00\n",
      "Episode: 320 Avg Time: 2.07s Avg Score: 7.69\n",
      "\t Episode: 340 Step: 250 Yellow Score: 06 Blue Score: 01 Score: 5.00\n",
      "Episode: 340 Avg Time: 2.07s Avg Score: 8.3\n",
      "\t Episode: 360 Step: 250 Yellow Score: 08 Blue Score: 00 Score: 8.00\n",
      "Episode: 360 Avg Time: 2.08s Avg Score: 8.39\n",
      "\t Episode: 380 Step: 250 Yellow Score: 10 Blue Score: 00 Score: 10.0\n",
      "Episode: 380 Avg Time: 2.08s Avg Score: 8.93\n",
      "\t Episode: 400 Step: 250 Yellow Score: 09 Blue Score: 01 Score: 8.00\n",
      "Episode: 400 Avg Time: 2.09s Avg Score: 9.14\n",
      "\t Episode: 420 Step: 250 Yellow Score: 11 Blue Score: 01 Score: 10.0\n",
      "Episode: 420 Avg Time: 2.09s Avg Score: 9.82\n",
      "\t Episode: 440 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 440 Avg Time: 2.09s Avg Score: 10.8\n",
      "\t Episode: 460 Step: 250 Yellow Score: 12 Blue Score: 01 Score: 11.0\n",
      "Episode: 460 Avg Time: 2.10s Avg Score: 11.63\n",
      "\t Episode: 480 Step: 250 Yellow Score: 12 Blue Score: 01 Score: 11.0\n",
      "Episode: 480 Avg Time: 2.10s Avg Score: 11.51\n",
      "\t Episode: 500 Step: 250 Yellow Score: 09 Blue Score: 00 Score: 9.00\n",
      "Episode: 500 Avg Time: 2.10s Avg Score: 10.86\n",
      "\t Episode: 520 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.00\n",
      "Episode: 520 Avg Time: 2.10s Avg Score: 9.74\n",
      "\t Episode: 540 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 540 Avg Time: 2.10s Avg Score: 8.82\n",
      "\t Episode: 560 Step: 250 Yellow Score: 09 Blue Score: 00 Score: 9.00\n",
      "Episode: 560 Avg Time: 2.11s Avg Score: 8.13\n",
      "\t Episode: 580 Step: 250 Yellow Score: 14 Blue Score: 00 Score: 14.0\n",
      "Episode: 580 Avg Time: 2.11s Avg Score: 8.48\n",
      "\t Episode: 600 Step: 250 Yellow Score: 11 Blue Score: 01 Score: 10.0\n",
      "Episode: 600 Avg Time: 2.11s Avg Score: 9.06\n",
      "\t Episode: 620 Step: 250 Yellow Score: 14 Blue Score: 01 Score: 13.0\n",
      "Episode: 620 Avg Time: 2.11s Avg Score: 10.26\n",
      "\t Episode: 640 Step: 250 Yellow Score: 06 Blue Score: 00 Score: 6.00\n",
      "Episode: 640 Avg Time: 2.11s Avg Score: 10.81\n",
      "\t Episode: 660 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 660 Avg Time: 2.11s Avg Score: 11.12\n",
      "\t Episode: 680 Step: 250 Yellow Score: 14 Blue Score: 00 Score: 14.0\n",
      "Episode: 680 Avg Time: 2.12s Avg Score: 10.49\n",
      "\t Episode: 700 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 700 Avg Time: 2.12s Avg Score: 10.16\n",
      "\t Episode: 720 Step: 250 Yellow Score: 13 Blue Score: 00 Score: 13.0\n",
      "Episode: 720 Avg Time: 2.12s Avg Score: 10.28\n",
      "\t Episode: 740 Step: 250 Yellow Score: 08 Blue Score: 00 Score: 8.00\n",
      "Episode: 740 Avg Time: 2.12s Avg Score: 10.17\n",
      "\t Episode: 760 Step: 250 Yellow Score: 03 Blue Score: 01 Score: 2.00\n",
      "Episode: 760 Avg Time: 2.12s Avg Score: 10.2\n",
      "\t Episode: 780 Step: 250 Yellow Score: 13 Blue Score: 01 Score: 12.0\n",
      "Episode: 780 Avg Time: 2.12s Avg Score: 11.02\n",
      "\t Episode: 800 Step: 250 Yellow Score: 10 Blue Score: 01 Score: 9.00\n",
      "Episode: 800 Avg Time: 2.12s Avg Score: 11.55\n",
      "\t Episode: 820 Step: 250 Yellow Score: 14 Blue Score: 01 Score: 13.0\n",
      "Episode: 820 Avg Time: 2.12s Avg Score: 11.12\n",
      "\t Episode: 840 Step: 250 Yellow Score: 13 Blue Score: 00 Score: 13.0\n",
      "Episode: 840 Avg Time: 2.12s Avg Score: 11.22\n",
      "\t Episode: 860 Step: 250 Yellow Score: 13 Blue Score: 00 Score: 13.0\n",
      "Episode: 860 Avg Time: 2.12s Avg Score: 11.56\n",
      "\t Episode: 880 Step: 250 Yellow Score: 08 Blue Score: 00 Score: 8.00\n",
      "Episode: 880 Avg Time: 2.13s Avg Score: 11.37\n",
      "\t Episode: 900 Step: 250 Yellow Score: 13 Blue Score: 00 Score: 13.0\n",
      "Episode: 900 Avg Time: 2.13s Avg Score: 11.24\n",
      "\t Episode: 920 Step: 250 Yellow Score: 07 Blue Score: 00 Score: 7.00\n",
      "Episode: 920 Avg Time: 2.13s Avg Score: 11.71\n",
      "\t Episode: 940 Step: 250 Yellow Score: 12 Blue Score: 01 Score: 11.0\n",
      "Episode: 940 Avg Time: 2.13s Avg Score: 12.23\n",
      "\t Episode: 960 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 960 Avg Time: 2.13s Avg Score: 12.1\n",
      "\t Episode: 980 Step: 250 Yellow Score: 08 Blue Score: 01 Score: 7.00\n",
      "Episode: 980 Avg Time: 2.13s Avg Score: 12.47\n",
      "\t Episode: 1000 Step: 250 Yellow Score: 02 Blue Score: 00 Score: 2.0\n",
      "Episode: 1000 Avg Time: 2.13s Avg Score: 12.41\n",
      "\t Episode: 1020 Step: 250 Yellow Score: 06 Blue Score: 01 Score: 5.00\n",
      "Episode: 1020 Avg Time: 2.13s Avg Score: 12.36\n",
      "\t Episode: 1040 Step: 250 Yellow Score: 15 Blue Score: 01 Score: 14.0\n",
      "Episode: 1040 Avg Time: 2.13s Avg Score: 12.02\n",
      "\t Episode: 1060 Step: 250 Yellow Score: 09 Blue Score: 01 Score: 8.00\n",
      "Episode: 1060 Avg Time: 2.13s Avg Score: 12.39\n",
      "\t Episode: 1080 Step: 250 Yellow Score: 11 Blue Score: 00 Score: 11.0\n",
      "Episode: 1080 Avg Time: 2.13s Avg Score: 12.25\n",
      "\t Episode: 1100 Step: 250 Yellow Score: 10 Blue Score: 00 Score: 10.0\n",
      "Episode: 1100 Avg Time: 2.14s Avg Score: 12.39\n",
      "\t Episode: 1120 Step: 250 Yellow Score: 07 Blue Score: 01 Score: 6.00\n",
      "Episode: 1120 Avg Time: 2.14s Avg Score: 12.61\n",
      "Solved environment in 1128 episodes, avg score: 13.01: 01 Score: 11.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an agent which interacts with the environment\n",
    "agent = Agent(state_size, action_size, SEED)\n",
    "\n",
    "if train_flag:\n",
    "\n",
    "    # Setup variables\n",
    "    all_scores = []\n",
    "    steps_period = int(steps_max/10)\n",
    "    episode_period = min([int(train_episodes/100), score_window])\n",
    "    start_time = time()\n",
    "\n",
    "    # Log info (such that browser can be safely closed)\n",
    "    text = 'priority_flag = {}\\nLR_decay_flag = {}\\nqualify_score = {}\\nscore_window = {}\\nBATCH_SIZE = {}\\n' + \\\n",
    "            'LR = {}\\nLR_DECAY_RATE = {}\\nLR_DECAY_STEP = {}\\nFC = {}\\n\\ntrain_episodes = {}\\n' + \\\n",
    "            'steps_max = {}\\neps_init = {}\\neps_decay = {}\\neps_min = {}\\nSAMPLE_IMP = {}\\nTAU = {}\\n' + \\\n",
    "            'UPDATE_EVERY = {}\\nBUFFER_SIZE = {}\\nGAMMA = {}\\n\\n'\n",
    "    text = text.format(priority_flag, LR_decay_flag, qualify_score, score_window, BATCH_SIZE, LR, LR_DECAY_RATE,\\\n",
    "                       LR_DECAY_STEP, FC, train_episodes, steps_max, eps_init, eps_decay, eps_min, \\\n",
    "                       SAMPLE_IMP, TAU, UPDATE_EVERY, BUFFER_SIZE,GAMMA)\n",
    "    with open(log_path,'w+') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    for e in range(train_episodes):\n",
    "\n",
    "        # Reset environment and setup variables\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0] # get the current state\n",
    "        score = 0 # initialize the score\n",
    "        yellow_bananas = 0\n",
    "        blue_bananas = 0\n",
    "        eps = eps_init # Initialize epsilon\n",
    "        solved_flag = False\n",
    "        if LR_decay_flag:\n",
    "            agent.scheduler_epoch = e\n",
    "\n",
    "        for t in range(steps_max):\n",
    "            # Get next action from the agent\n",
    "            action = agent.act(state, eps)\n",
    "\n",
    "            # Run one step and collect env info\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            if reward == 1:\n",
    "                yellow_bananas += 1\n",
    "            elif reward == -1:\n",
    "                blue_bananas += 1\n",
    "            score += reward\n",
    "\n",
    "            # Check if required average score is obtained\n",
    "            if t % steps_period == 0:\n",
    "                text = \"\\t Episode: {} Step: {:03} Yellow Score: {:02} Blue Score: {:02} Score: {:02}\".format(\\\n",
    "                    e, t, yellow_bananas, blue_bananas, score)\n",
    "                print(text, end = '\\r')\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "            if done:\n",
    "                    all_scores.append(score)\n",
    "                    break\n",
    "\n",
    "            # Prepare for the next step\n",
    "            agent.step(state, action, reward, next_state, done) # Store the information of last step\n",
    "            state = next_state # Update state\n",
    "            eps = max(eps_min, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "\n",
    "        # Check if required average score is obtained\n",
    "        if len(all_scores) > score_window:\n",
    "            if e % episode_period == 0:\n",
    "                text = \"Episode: {} Avg Time: {:0.2f}s Avg Score: {}\".format(e, (time()-start_time)/e, \\\n",
    "                                                                            np.mean(all_scores[-score_window:]))\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "                print()\n",
    "                print(text)\n",
    "                \n",
    "                tc.save(agent.dqn_local.state_dict(), 'checkpoint_unoptimized.pth')\n",
    "\n",
    "                # Plot the scores - for better viz, than just texts\n",
    "                fig = plt.figure()\n",
    "                plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "                plt.ylabel('Score')\n",
    "                plt.xlabel('Episode #')\n",
    "                plt.grid()\n",
    "                fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "                plt.close(fig)\n",
    "                \n",
    "            if np.mean(all_scores[-score_window:]) > qualify_score:\n",
    "                text = \"Solved environment in {} episodes, avg score: {}\".format(\\\n",
    "                    e, np.mean(all_scores[-score_window:]))\n",
    "\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "                \n",
    "                print(text)\n",
    "                \n",
    "                tc.save(agent.dqn_local.state_dict(), 'checkpoint.pth')\n",
    "                \n",
    "                solved_flag = True\n",
    "                \n",
    "                break\n",
    "                \n",
    "        \n",
    "    # Plot the scores\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    if solved_flag:\n",
    "        plt.title(text)\n",
    "    else:\n",
    "        plt.title(text[2:])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "    fig.savefig('rewards.png', bbox_inches='tight') # Raster graphics to show later in Section 9\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Test the performance (only if `train_flag=False`)\n",
    "\n",
    "Test an already trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_flag:\n",
    "    \n",
    "    # Initialization\n",
    "    agent.dqn_local.load_state_dict(tc.load('checkpoint.pth')) # load the weights from file\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0] # get the current state\n",
    "    score = 0 # initialize the score\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state,0.0) # select an action\n",
    "        env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0] # get the next state\n",
    "        reward = env_info.rewards[0] # get the reward\n",
    "        done = env_info.local_done[0] # see if episode has finished\n",
    "        score += reward # update the score\n",
    "        state = next_state # roll over the state to next time step\n",
    "        if done: # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### Plot of training rewards\n",
    "\n",
    "![banana_agent_rewards](rewards.png)\n",
    "\n",
    "NOTE: The above figure might not be the true figure due to disk cache problem. Please see `fig_path` file for the latest and true graph.\n",
    "\n",
    "### Observations\n",
    "\n",
    "There were a lot of observations and learnings during the training sessions:\n",
    "- In this work, a Double DQN approach with Prioritized Experience Replay Buffer is implemented. This is because from personal exploration, a simple DQN didn't seem to train steadily i.e. avg score kept fluctuating a lot. Furthermore, in general, a random sampling in the replay buffer provided the torch optimizer with experience in which reward is typically 0 i.e. an experience which doesn't teach much useful information.\n",
    "- Introduction of softmax to the output of the neural network tremendously reduces the\n",
    "    performance of the learning. This is indeed surprising because typically the probabilistic output networks\n",
    "    have softmax to ensure that output is probabilistic (i.e. 1<p<1 and sum(p) = 1).\n",
    "- Input normalization, specifically the linear and angular velocity of the agent to [-1, 1], \n",
    "    doesn't seem to help with network learning.\n",
    "- Learning rate decay of the network optimizer doesn't seem to help in improving learning rate,\n",
    "    probably because this require precise tuning of the decay rate and initial value of the learning rate.\n",
    "- Prioritized Replay Buffer helps increase the rate of learning substantially, although at the cost of\n",
    "    computational time.\n",
    "  \n",
    "- The hyperparameters which suit well are designed such that the local network trains faster\n",
    "    but slightly unstably, while the target network trains slowly and tries to make the prediction\n",
    "    much more stable.\n",
    "- There were numerous cases where the training performance increased rapidly, then decreased, followed by increase again in the average score. Such a situation might arise due to fast learning approach such as target network is updated to local network very fast, while the local network is being trained with small batch size with minimal data.\n",
    "- For instance, in one of the sets of the training parameters, the avg score of +13 over 100 episodes was obtained within about 500 episodes. Although, the score graph was very noisy and the agent's performace during testing was very jerky, almost as if the agent was randomly choosing actions instead of continuously progressing towards a yellow banana.\n",
    "- In constrast to above, several cases where also observed where the system doesn't learn much even after thousands of training episodes, simply because the training parameters were tuned to learn slowly. Accordingly, it is learned that the training parameters can significantly affect the system performance, beyond the learning algorithm itself.\n",
    "- A lot of times, the average score increased and thereafter decrease after a number of episodes. This might be because the network is trained to learn faster, rather than learn a lot of good moves.\n",
    "- It is also learned that there needs to be a trade-off between larger buffer size against slower convergence of target network to the local network, because otherwise, the network might forget good experiences very easily and start to work on poor results.\n",
    "- Finally, a few trials with random initial network weights is a good approach to ensure trying out all potentially good possible results.\n",
    "\n",
    "### Future Ideas\n",
    "\n",
    "There were a lot of learning from our observations as mentioned above.\n",
    "Luckily, there does exists a lot of algorithms (such as Actor-Critic approach, DDPG, etc.) in the literature which focus on overcoming the short-coming of the Deep Q-Network approach implemented in this work.\n",
    "Hence, in future, it is worth exploring other algorithms for improving the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
